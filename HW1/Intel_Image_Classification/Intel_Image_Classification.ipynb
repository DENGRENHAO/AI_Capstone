{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a761c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d puneet6060/intel-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51fc2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, plot_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation, RandomZoom, Dense, Flatten, Dropout, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7800588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from zipfile import ZipFile\n",
    "# file_name=\"./dataset/intel-image-classification.zip\"\n",
    "\n",
    "# with ZipFile(file_name,'r') as zip:\n",
    "#     zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da5e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset path\n",
    "train_path = './dataset/seg_train/seg_train/'\n",
    "test_path = './dataset/seg_test/seg_test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f395e",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f3f172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_model():\n",
    "    # CNN model architecture\n",
    "    model = Sequential([\n",
    "        Conv2D(16, 3, padding='same', activation='relu', input_shape=(150, 150, 3)),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(6, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff8b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to type tensorflow PrefetchDataset\n",
    "def load_train_validation_dataset(path, seed):\n",
    "    train_ds = image_dataset_from_directory(\n",
    "        directory=path,\n",
    "        seed=seed,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        image_size=(150, 150),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_ds = image_dataset_from_directory(\n",
    "        directory=path,\n",
    "        seed=seed,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        image_size=(150, 150),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2315b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to type tensorflow PrefetchDataset\n",
    "def load_test_dataset(path, seed):\n",
    "    test_ds = image_dataset_from_directory(\n",
    "        directory=path,\n",
    "        image_size=(150, 150),\n",
    "        shuffle=False\n",
    "    )\n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122c1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_ds, val_ds, test_ds):\n",
    "    # Rescale pixel value from 0~255 to 0~1\n",
    "    scaling = tf.keras.layers.Rescaling(1./255)\n",
    "    train_ds = train_ds.map(lambda x, y: (scaling(x), y))\n",
    "    val_ds = val_ds.map(lambda x, y: (scaling(x), y))\n",
    "    test_ds = test_ds.map(lambda x, y: (scaling(x), y))\n",
    "\n",
    "    # Make training process faster\n",
    "    train_ds = train_ds.cache().shuffle(buffer_size=64).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().shuffle(buffer_size=64).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa78835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test dataset's label from PrefetchDataset and return numpy array\n",
    "def get_test_label(test_ds):\n",
    "    test_data = [(image.numpy(), label.numpy()) for image, label in test_ds]\n",
    "    test_label = test_data[0][1]\n",
    "    for i in range(1, len(test_data)):\n",
    "        test_label = np.concatenate([test_label, test_data[i][1]], axis=0)\n",
    "    \n",
    "    return list(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe98e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn probability prediction to label\n",
    "def get_prediction_label(prediction):\n",
    "    predict_label = np.argmax(prediction, axis=1)\n",
    "    \n",
    "    return list(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443959ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_results(test_label, predict_label):\n",
    "    metric_results = {}\n",
    "    metric_results['accuracy'] = accuracy_score(test_label, predict_label)\n",
    "    metric_results['f1_score'] = f1_score(test_label, predict_label, average=\"macro\")\n",
    "    metric_results['precision'] = precision_score(test_label, predict_label, average=\"macro\")\n",
    "    metric_results['recall'] = recall_score(test_label, predict_label, average=\"macro\")\n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec0d8d88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "351/351 [==============================] - 5s 13ms/step - loss: 1.0712 - accuracy: 0.5765 - val_loss: 0.7987 - val_accuracy: 0.7014\n",
      "Epoch 2/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.7705 - accuracy: 0.7162 - val_loss: 0.7235 - val_accuracy: 0.7452\n",
      "Epoch 3/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.6471 - accuracy: 0.7646 - val_loss: 0.5853 - val_accuracy: 0.7947\n",
      "Epoch 4/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5515 - accuracy: 0.8015 - val_loss: 0.5524 - val_accuracy: 0.8097\n",
      "Epoch 5/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4752 - accuracy: 0.8293 - val_loss: 0.6374 - val_accuracy: 0.7812\n",
      "Epoch 6/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4019 - accuracy: 0.8567 - val_loss: 0.5610 - val_accuracy: 0.8168\n",
      "Epoch 7/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.3526 - accuracy: 0.8743 - val_loss: 0.5708 - val_accuracy: 0.8211\n",
      "Epoch 8/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.3046 - accuracy: 0.8902 - val_loss: 0.6243 - val_accuracy: 0.8307\n",
      "Epoch 9/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.2702 - accuracy: 0.9038 - val_loss: 0.6486 - val_accuracy: 0.8225\n",
      "Epoch 10/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2410 - accuracy: 0.9157 - val_loss: 0.7466 - val_accuracy: 0.8168\n",
      "94/94 [==============================] - 1s 9ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "351/351 [==============================] - 5s 13ms/step - loss: 1.0713 - accuracy: 0.5893 - val_loss: 0.7624 - val_accuracy: 0.7167\n",
      "Epoch 2/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.7450 - accuracy: 0.7261 - val_loss: 0.5898 - val_accuracy: 0.7766\n",
      "Epoch 3/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.6024 - accuracy: 0.7810 - val_loss: 0.5356 - val_accuracy: 0.8029\n",
      "Epoch 4/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5021 - accuracy: 0.8161 - val_loss: 0.5328 - val_accuracy: 0.8111\n",
      "Epoch 5/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4216 - accuracy: 0.8463 - val_loss: 0.6080 - val_accuracy: 0.7758\n",
      "Epoch 6/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.3576 - accuracy: 0.8709 - val_loss: 0.5470 - val_accuracy: 0.8054\n",
      "Epoch 7/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2972 - accuracy: 0.8921 - val_loss: 0.5534 - val_accuracy: 0.8190\n",
      "Epoch 8/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2508 - accuracy: 0.9110 - val_loss: 0.5683 - val_accuracy: 0.8197\n",
      "Epoch 9/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.2266 - accuracy: 0.9190 - val_loss: 0.5598 - val_accuracy: 0.8254\n",
      "Epoch 10/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.1712 - accuracy: 0.9400 - val_loss: 0.6980 - val_accuracy: 0.8193\n",
      "94/94 [==============================] - 1s 9ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "351/351 [==============================] - 5s 13ms/step - loss: 1.0453 - accuracy: 0.5924 - val_loss: 0.7742 - val_accuracy: 0.7035\n",
      "Epoch 2/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.7239 - accuracy: 0.7330 - val_loss: 0.7020 - val_accuracy: 0.7320\n",
      "Epoch 3/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5708 - accuracy: 0.7963 - val_loss: 0.5391 - val_accuracy: 0.8086\n",
      "Epoch 4/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4786 - accuracy: 0.8305 - val_loss: 0.5426 - val_accuracy: 0.7962\n",
      "Epoch 5/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.3879 - accuracy: 0.8561 - val_loss: 0.5543 - val_accuracy: 0.8086\n",
      "Epoch 6/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.3252 - accuracy: 0.8837 - val_loss: 0.5512 - val_accuracy: 0.8047\n",
      "Epoch 7/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2813 - accuracy: 0.8956 - val_loss: 0.5441 - val_accuracy: 0.8215\n",
      "Epoch 8/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2268 - accuracy: 0.9198 - val_loss: 0.6109 - val_accuracy: 0.8168\n",
      "Epoch 9/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.1972 - accuracy: 0.9312 - val_loss: 0.7457 - val_accuracy: 0.8001\n",
      "Epoch 10/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.1656 - accuracy: 0.9424 - val_loss: 0.6882 - val_accuracy: 0.8300\n",
      "94/94 [==============================] - 1s 9ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "351/351 [==============================] - 5s 13ms/step - loss: 1.0526 - accuracy: 0.5924 - val_loss: 0.8226 - val_accuracy: 0.6932\n",
      "Epoch 2/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.7559 - accuracy: 0.7193 - val_loss: 0.6455 - val_accuracy: 0.7655\n",
      "Epoch 3/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5913 - accuracy: 0.7889 - val_loss: 0.5492 - val_accuracy: 0.8108\n",
      "Epoch 4/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.5018 - accuracy: 0.8251 - val_loss: 0.5671 - val_accuracy: 0.8090\n",
      "Epoch 5/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4109 - accuracy: 0.8541 - val_loss: 0.5700 - val_accuracy: 0.8111\n",
      "Epoch 6/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.3465 - accuracy: 0.8773 - val_loss: 0.5832 - val_accuracy: 0.8175\n",
      "Epoch 7/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2957 - accuracy: 0.8943 - val_loss: 0.6153 - val_accuracy: 0.8158\n",
      "Epoch 8/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2442 - accuracy: 0.9113 - val_loss: 0.6227 - val_accuracy: 0.8261\n",
      "Epoch 9/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2046 - accuracy: 0.9240 - val_loss: 0.6536 - val_accuracy: 0.8225\n",
      "Epoch 10/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.1788 - accuracy: 0.9374 - val_loss: 0.7647 - val_accuracy: 0.8140\n",
      "94/94 [==============================] - 1s 9ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "351/351 [==============================] - 5s 12ms/step - loss: 1.0631 - accuracy: 0.5866 - val_loss: 0.7668 - val_accuracy: 0.7170\n",
      "Epoch 2/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.7600 - accuracy: 0.7178 - val_loss: 0.6534 - val_accuracy: 0.7609\n",
      "Epoch 3/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.6123 - accuracy: 0.7791 - val_loss: 0.5948 - val_accuracy: 0.7705\n",
      "Epoch 4/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4859 - accuracy: 0.8273 - val_loss: 0.5423 - val_accuracy: 0.7958\n",
      "Epoch 5/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.4166 - accuracy: 0.8525 - val_loss: 0.5685 - val_accuracy: 0.8079\n",
      "Epoch 6/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.3382 - accuracy: 0.8806 - val_loss: 0.5607 - val_accuracy: 0.8218\n",
      "Epoch 7/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2858 - accuracy: 0.8994 - val_loss: 0.5630 - val_accuracy: 0.8200\n",
      "Epoch 8/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.2310 - accuracy: 0.9173 - val_loss: 0.6191 - val_accuracy: 0.8243\n",
      "Epoch 9/10\n",
      "351/351 [==============================] - 3s 9ms/step - loss: 0.1995 - accuracy: 0.9266 - val_loss: 0.6628 - val_accuracy: 0.8382\n",
      "Epoch 10/10\n",
      "351/351 [==============================] - 3s 10ms/step - loss: 0.1766 - accuracy: 0.9379 - val_loss: 0.7115 - val_accuracy: 0.8286\n",
      "94/94 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "f1score = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for i in range(5):\n",
    "    seed = i ** 2\n",
    "    train_ds, val_ds = load_train_validation_dataset(train_path, seed)\n",
    "    test_ds = load_test_dataset(test_path, seed)\n",
    "    train_ds, val_ds, test_ds = preprocessing(train_ds, val_ds, test_ds)\n",
    "    \n",
    "    model = get_CNN_model()\n",
    "    model.fit(train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=10)\n",
    "    \n",
    "    prediction = model.predict(test_ds)\n",
    "    \n",
    "    test_label = get_test_label(test_ds)\n",
    "    predict_label = get_prediction_label(prediction)\n",
    "    \n",
    "    metric_result = get_metric_results(test_label, predict_label)\n",
    "    \n",
    "    accuracy.append(metric_result['accuracy'])\n",
    "    f1score.append(metric_result['f1_score'])\n",
    "    precision.append(metric_result['precision'])\n",
    "    recall.append(metric_result['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ee6242c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: ['0.81', '0.81', '0.83', '0.83', '0.82']\n",
      "F1score: ['0.82', '0.81', '0.83', '0.83', '0.82']\n",
      "Precision: ['0.82', '0.81', '0.84', '0.83', '0.82']\n",
      "Recall: ['0.82', '0.81', '0.83', '0.83', '0.82']\n",
      "Average accuracy: 0.82\n",
      "Average f1score: 0.821073622157208\n",
      "Average precision: 0.825036380671853\n",
      "Average recall: 0.8212334485802897\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', ['%.2f' % val for val in accuracy])\n",
    "print('F1score:', ['%.2f' % val for val in f1score])\n",
    "print('Precision:', ['%.2f' % val for val in precision])\n",
    "print('Recall:', ['%.2f' % val for val in recall])\n",
    "print('Average accuracy:', sum(accuracy) / len(accuracy))\n",
    "print('Average f1score:', sum(f1score) / len(f1score))\n",
    "print('Average precision:', sum(precision) / len(precision))\n",
    "print('Average recall:', sum(recall) / len(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71927a",
   "metadata": {},
   "source": [
    "## Transfer Learning VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c08cdf50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_VGG16_pretrained_model():\n",
    "    pretrained_model = VGG16(input_shape = (150, 150, 3), \n",
    "                             include_top = False, \n",
    "                             weights = 'imagenet')\n",
    "\n",
    "    for layer in pretrained_model.layers:\n",
    "         layer.trainable = False\n",
    "    \n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406b5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfered_model(last_layer_output, pretrained_model_input):\n",
    "    x = Flatten()(last_layer_output)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)                  \n",
    "    x = Dense(6, activation='softmax')(x)           \n",
    "    model = Model(pretrained_model_input, x)\n",
    "    \n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001), \n",
    "              loss = 'sparse_categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "428efe95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "351/351 [==============================] - 18s 44ms/step - loss: 0.5057 - accuracy: 0.8125 - val_loss: 0.3751 - val_accuracy: 0.8635\n",
      "Epoch 2/5\n",
      "351/351 [==============================] - 11s 33ms/step - loss: 0.3383 - accuracy: 0.8782 - val_loss: 0.3627 - val_accuracy: 0.8724\n",
      "Epoch 3/5\n",
      "351/351 [==============================] - 11s 33ms/step - loss: 0.2755 - accuracy: 0.8986 - val_loss: 0.3516 - val_accuracy: 0.8778\n",
      "Epoch 4/5\n",
      "351/351 [==============================] - 11s 33ms/step - loss: 0.2337 - accuracy: 0.9165 - val_loss: 0.3554 - val_accuracy: 0.8849\n",
      "Epoch 5/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.1963 - accuracy: 0.9306 - val_loss: 0.3640 - val_accuracy: 0.8817\n",
      "94/94 [==============================] - 3s 36ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "351/351 [==============================] - 14s 38ms/step - loss: 0.5156 - accuracy: 0.8117 - val_loss: 0.3675 - val_accuracy: 0.8642\n",
      "Epoch 2/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.3411 - accuracy: 0.8788 - val_loss: 0.3672 - val_accuracy: 0.8649\n",
      "Epoch 3/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2808 - accuracy: 0.9010 - val_loss: 0.4586 - val_accuracy: 0.8450\n",
      "Epoch 4/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2370 - accuracy: 0.9149 - val_loss: 0.3562 - val_accuracy: 0.8806\n",
      "Epoch 5/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.1998 - accuracy: 0.9294 - val_loss: 0.3293 - val_accuracy: 0.8874\n",
      "94/94 [==============================] - 3s 26ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "351/351 [==============================] - 15s 38ms/step - loss: 0.5177 - accuracy: 0.8105 - val_loss: 0.3447 - val_accuracy: 0.8763\n",
      "Epoch 2/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.3366 - accuracy: 0.8786 - val_loss: 0.3702 - val_accuracy: 0.8667\n",
      "Epoch 3/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2731 - accuracy: 0.9032 - val_loss: 0.3667 - val_accuracy: 0.8642\n",
      "Epoch 4/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2343 - accuracy: 0.9147 - val_loss: 0.3680 - val_accuracy: 0.8735\n",
      "Epoch 5/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.1953 - accuracy: 0.9304 - val_loss: 0.3298 - val_accuracy: 0.8860\n",
      "94/94 [==============================] - 3s 26ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "351/351 [==============================] - 15s 38ms/step - loss: 0.5093 - accuracy: 0.8126 - val_loss: 0.3726 - val_accuracy: 0.8560\n",
      "Epoch 2/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.3361 - accuracy: 0.8793 - val_loss: 0.4050 - val_accuracy: 0.8535\n",
      "Epoch 3/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2787 - accuracy: 0.8982 - val_loss: 0.3567 - val_accuracy: 0.8649\n",
      "Epoch 4/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2345 - accuracy: 0.9131 - val_loss: 0.3599 - val_accuracy: 0.8713\n",
      "Epoch 5/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.1987 - accuracy: 0.9276 - val_loss: 0.3668 - val_accuracy: 0.8685\n",
      "94/94 [==============================] - 3s 26ms/step\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 11228 files for training.\n",
      "Found 14034 files belonging to 6 classes.\n",
      "Using 2806 files for validation.\n",
      "Found 3000 files belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "351/351 [==============================] - 14s 38ms/step - loss: 0.5139 - accuracy: 0.8107 - val_loss: 0.3543 - val_accuracy: 0.8664\n",
      "Epoch 2/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.3375 - accuracy: 0.8799 - val_loss: 0.3634 - val_accuracy: 0.8785\n",
      "Epoch 3/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.2767 - accuracy: 0.8997 - val_loss: 0.3451 - val_accuracy: 0.8785\n",
      "Epoch 4/5\n",
      "351/351 [==============================] - 12s 34ms/step - loss: 0.2324 - accuracy: 0.9177 - val_loss: 0.3442 - val_accuracy: 0.8770\n",
      "Epoch 5/5\n",
      "351/351 [==============================] - 12s 33ms/step - loss: 0.1965 - accuracy: 0.9279 - val_loss: 0.4258 - val_accuracy: 0.8510\n",
      "94/94 [==============================] - 2s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "VGG_accuracy = []\n",
    "VGG_f1score = []\n",
    "VGG_precision = []\n",
    "VGG_recall = []\n",
    "\n",
    "pretrained_model = get_VGG16_pretrained_model()\n",
    "last_layer = pretrained_model.get_layer('block5_pool')\n",
    "\n",
    "for i in range(5):\n",
    "    seed = i ** 2\n",
    "    train_ds, val_ds = load_train_validation_dataset(train_path, seed)\n",
    "    test_ds = load_test_dataset(test_path, seed)\n",
    "    train_ds, val_ds, test_ds = preprocessing(train_ds, val_ds, test_ds)\n",
    "    \n",
    "    model = get_transfered_model(last_layer.output, pretrained_model.input)\n",
    "    \n",
    "    model.fit(train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=5)\n",
    "    \n",
    "    prediction = model.predict(test_ds)\n",
    "    \n",
    "    test_label = get_test_label(test_ds)\n",
    "    predict_label = get_prediction_label(prediction)\n",
    "    \n",
    "    metric_result = get_metric_results(test_label, predict_label)\n",
    "    \n",
    "    VGG_accuracy.append(metric_result['accuracy'])\n",
    "    VGG_f1score.append(metric_result['f1_score'])\n",
    "    VGG_precision.append(metric_result['precision'])\n",
    "    VGG_recall.append(metric_result['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "405c984c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: ['0.88', '0.88', '0.88', '0.87', '0.85']\n",
      "F1score: ['0.88', '0.89', '0.88', '0.88', '0.85']\n",
      "Precision: ['0.88', '0.89', '0.89', '0.88', '0.88']\n",
      "Recall: ['0.88', '0.89', '0.88', '0.88', '0.85']\n",
      "Average accuracy: 0.8735333333333333\n",
      "Average f1score: 0.8759250985478586\n",
      "Average precision: 0.8823271380573475\n",
      "Average recall: 0.8764593897398842\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', ['%.2f' % val for val in VGG_accuracy])\n",
    "print('F1score:', ['%.2f' % val for val in VGG_f1score])\n",
    "print('Precision:', ['%.2f' % val for val in VGG_precision])\n",
    "print('Recall:', ['%.2f' % val for val in VGG_recall])\n",
    "print('Average accuracy:', sum(VGG_accuracy) / len(VGG_accuracy))\n",
    "print('Average f1score:', sum(VGG_f1score) / len(VGG_f1score))\n",
    "print('Average precision:', sum(VGG_precision) / len(VGG_precision))\n",
    "print('Average recall:', sum(VGG_recall) / len(VGG_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd5be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
